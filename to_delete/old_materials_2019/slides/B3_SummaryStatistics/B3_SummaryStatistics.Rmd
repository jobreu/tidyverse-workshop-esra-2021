---
title: "Data Wrangling & Exploration with the Tidyverse in R"
subtitle: "Summary Statistics"
author: "Johannes Breuer<br />Stefan JÃ¼nger<br />Thomas Ebel"
date: "2018-05-16"
location: "GESIS, Mannheim, Germany"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["default", "default-fonts", "../workshop.css"]
    nature:
      highlightStyle: "github"
      highlightLines: true
      countIncrementalSlides: false
---
layout: true

```{r setup, include=FALSE}
if (!require(easypackages)) install.packages("easypackages")
library(easypackages)

packages(
  "knitr", "rmarkdown", "tidyverse", "readxl", "datasauRus", "hadley/emo",
  prompt = FALSE
)

options(htmltools.dir.version = FALSE)

opts_chunk$set(echo = FALSE, fig.align = "center")
set.seed(123)
```

<div class="my-footer">
  <div style="float: left;"><span>`r gsub("<br />", " & ", gsub("<br /><br />|<a.+$", "", metadata$author))`</span></div>
  <div style="float: right;"><span>`r metadata$location`, `r metadata$date`</span></div>
  <div style="text-align: center;"><span>`r gsub(".+<br />", " ", metadata$title)`</span></div>
</div>

---

# Understanding data is hard 

If we look at raw numbers, we might have a hard time understanding them.

--

Interpreting single numbers is easy, but it's almost impossible, to say which of the following two vectors has

- more variance or
- the lower mean

just by looking at them:

--

```{r example vectors, results = 'markup'}
x = sample(x = 18:65, size = 10, replace = TRUE)
y = sample(x = 20:60, size = 10, replace = TRUE)

(x)
(y)
```

---

# Making sense of data

To make sense of data we reduce their information to unique values.

--

.center[
~ 

**That's a simple definition of summary statistics**

~]

--

As such, we use summarizing functions of
- location (e.g., the mean),
- spread (e.g., standard deviation),
- the shape of the distribution (e.g., skewness), and
- relations between variables (e.g., correlation coefficients)

--

We assume everyone here knows the basics of summary statistics. So instead of their repeating them, let's see how to create them with tools from the `tidyverse`.

---

# The `summarize()` function

`dplyr` provides a helpful function for creating summary statistics: `summarize()`

--

It's a vecorized function that can be used to create summary statistics for variables in the non-standard evaluation format for all standard `R` functions, such as:

- `mean()`

- `sd()`

- `min()`

- `max()`

- ...


---

# A simple example

```{r example data, echo = T}
tibble(
  x = sample(x = 18:65, size = 10, replace = TRUE),
  y = sample(x = 20:60, size = 10, replace = TRUE)
) %>% 
  summarize(
    mean_x = mean(x), 
    mean_y = mean(y),
    var_x = sd(x)^2, 
    var_y = sd(y)^2
  )
```

--

The underlying vectors of this tibble are short. In the real world, vectors are also tend to be too long to be eyeballed conveniently.

---

# Gapminder data

As in many of the previous exercises we will use data from [*Gapminder*](https://www.gapminder.org/) in the following. Our focal variable is income per person (fixed 2000 US$) in 275 countries for the years 1960-2011:


> "Gross Domestic Product per capita in constant 2000 US$. The inflation but not the differences in the cost of living between countries has been taken into account."

--

```{r load gdp data, echo = T}
capita <- 
  read_excel(
    path = "../../data/gapminder/GDPpercapitaconstant2000US.xlsx", 
    sheet = "Data",
    col_names = TRUE 
    )
```

---

# A first glimpse at the data

```{r glimpse capita}
capita %>% 
  rename(country = 1) %>% 
  glimpse()
```

---

# Tidying & wrangling the data

Let's start by reducing the data to 3 countries and converting them to the long format. All of this should look familiar to you by now.

--

```{r capita_long_subset, echo = TRUE}
capita_long <- 
  capita %>%
  rename(country = 1) %>%
  filter(
    country %in% 
      c("Germany", "United Kingdom", "United States")
  ) %>%
  gather(-1, key = "year", value = "income") %>%
  arrange(country, year)
```

---

# How does it look?

We display only the tail (last 6 observations here) because Germany, which appears at the head of the new dataframe, has missings up to and including the year 1969.

--

```{r capita_long_subset_look}
capita_long %>% tail()
```

--

In any case, information is reduced, and the data look much nicer now.

---

# The `group_by()` function

With the `group_by()` function from the `dplyr` package we can perform operations 'by group'. This is especially helpful if you want to compute separate summary statistics with `summarize()` for different groups in your dataset.

Note: If you want to remove the grouping from your dataframe, you can use `ungroup()`.

---

# Time for some summary statistics

```{r summary gdp, echo = T}
capita_long %>%
  mutate(change = (income / lag(income) - 1) * 100) %>%
  group_by(country) %>% #<<
  summarise(
    n = sum(!is.na(income)), 
    min_inc = min(income, na.rm = TRUE),
    max_inc = max(income, na.rm = TRUE), 
    diff = max_inc - min_inc, has_na = any(is.na(income)),
    max_change = max(change, na.rm = TRUE)
  )
```

<span class="footnote">
  Note: `change` is the change in income from the previous measurement in percent.
</span>

---

# Searching for extreme values

We may, for example, want to find years in which the change in income was higher than 7% (as the maximum value was `7.11`).

--

```{r extreme values, echo = T}
capita_long %>%
  mutate(change = (income / lag(income) - 1) * 100) %>%
  filter(change > 7) 
```

---

# Finding the year with the maximum value

```{r max value, echo = T}
capita_long %>% 
  mutate(change = (income / lag(income) - 1) * 100) %>%  
  filter(change == max(change, na.rm = TRUE))
```

--

With this code, we see the data for the country and year in which the GDP showed the highest increase in our dataset. This can be helpful even if we do not care about (other) summary statistics at all.

---

# But, beware: summary statistics can also be misleading! 

```{r fool, out.width = "50%"}
include_graphics("./pics/pity_the_fool.jpg")
```

---

# Misleading correlation coefficients

.column-left-half[
For example, if we do correlation analysis and we encounter a (Pearson's) correlation coefficient close to 0, we often think of relationships as pictured on the right side.

- It's noisy

- There's no observable pattern

- ...it's just a mess, and we question our life decision of working with data
]

.column-right-half[
```{r dino plot 1, out.width = "80%"}
datasauRus::datasaurus_dozen %>% 
  filter(dataset == "h_lines") %>% 
  ggplot(aes(x = x, y = y)) + geom_point() + theme_void()
```
]

---

# Do not despair

The [`datasauRus` package](https://github.com/lockedata/datasauRus) (which basically is an extension of [Anscombe's quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet)) proves to us that various different relations with the same summary statistics are possible.

--

This dataset has **the same correlation coefficient (-0.06, Pearson's)** as the one in the previous plot:

```{r dino plot 2, out.width = "40%"}
datasauRus::datasaurus_dozen %>% 
  filter(dataset == "slant_up") %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point() + 
  theme_void()
```

--

.center[But there's a trajectory, right? `r ji("dollar")`]

---

# What about this one? (r = -0.06)

```{r dino plot 3}
datasauRus::datasaurus_dozen %>% 
  filter(dataset == "dino") %>% 
  ggplot(aes(x = x, y = y)) + geom_point() + theme_void()
```

---

# What about this one? (r = -0.06)

.column-left-half[  
```{r roar, out.width = "60%"}
include_graphics("./pics/rooooar.png")
```
]

.column-right-half[
```{r dino plot 4}
datasauRus::datasaurus_dozen %>% 
  filter(dataset == "dino") %>% 
  ggplot(aes(x = x, y = y)) + geom_point() + theme_void()
```
]

---

# And it goes on and on...

All of these 13 datasets have the same means, standard deviations, and Pearson's correlation coefficients.

```{r dino plot 5, out.width = "55%"}
ggplot(
  datasaurus_dozen, 
  aes(
    x = x, 
    y = y, 
    colour = dataset
  )
) +
  geom_point() +
  theme_void() +
  theme(legend.position = "none") +
  facet_wrap(~dataset, ncol=3)
```

---

# Let's prove that to ourselves!

Here's what the data look like

```{r dino glimpse, echo = T}
datasauRus::datasaurus_dozen %>% 
  glimpse()
```

---

# Let's prove that to ourselves!

Maybe, in this case, it's more convenient to count the data points in each dataset.

```{r dino table, echo = T}
datasauRus::datasaurus_dozen %>% 
  select(dataset) %>% 
  table()
```

---

# Are the points identical across the datasets?

Here's how it works:

```{r identical, echo = T}
x <- c(1,2,3,4)
x2 <- c(4,3,2,1)
identical(sort(x),sort(x2))
```


---
# Are the points identical across the datasets?

And for example, for the dataset `dino` and `away`:

```{r dino identical, echo = T}
dino <- 
  datasaurus_dozen %>%
  filter(dataset == "dino")

away <- 
  datasaurus_dozen %>%
  filter(dataset == "away")

identical(sort(dino$x), sort(away$x))
```


---

# Let's compute the summary statistics

```{r datasaurus_solution, eval = FALSE, echo=TRUE}
datasaurus_dozen %>% 
  group_by(dataset) %>%
  summarise(
    n = n(), 
    mean_x = mean(x), 
    mean_y = mean(y), 
    sd_x = sd(x), 
    sd_y = sd(y), 
    corr = cor(x, y, method = "pearson")
  )
```

---

# Let's compute the summary statistics

```{r ref.label = "datasaurus_solution"}
```

---

class: center, middle

# [Exercise](https://jobreu.github.io/tidyverse-workshop-gesis-2019/exercises/B3_SummaryStatistics_exercises_question.html) time `r ji("weight_lifting_woman")``r ji("muscle")``r ji("running_man")``r ji("biking_man")`

## [Solutions](https://jobreu.github.io/tidyverse-workshop-gesis-2019/solutions/B3_SummaryStatistics_exercises_solution.html)